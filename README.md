# ğŸ•·ï¸ Web Crawler Chatbot

> **A fully functional RAG-based web crawler that scrapes websites and answers questions about the crawled content.**

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=flat&logo=python&logoColor=white)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?style=flat&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![React](https://img.shields.io/badge/React-18+-61DAFB?style=flat&logo=react&logoColor=black)](https://reactjs.org)

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ•·ï¸ **Web Crawler** | Crawls websites using Playwright with configurable depth & page limits |
| ğŸ§  **RAG Pipeline** | Splits text into chunks, generates embeddings, stores in vector database |
| ğŸ’¬ **Chat Interface** | Clean chat UI with source attribution |
| âš™ï¸ **Admin Panel** | Web-based content ingestion interface |
| ğŸš€ **Zero Cost** | Groq LLM (free) + local embeddings + ChromaDB (local) |
| ğŸ“ **File Upload** | Support for PDF and text file ingestion |

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **Node.js 18+**
- **[Groq API Key](https://console.groq.com)** (free)

### Installation

```bash
# Clone the repository
git clone https://github.com/Udai-Senevirathne/Web-Crawling.git
cd Web-Crawling

# Create & activate virtual environment
python -m venv .venv
.venv\Scripts\activate        # Windows
# source .venv/bin/activate   # Linux/Mac

# Install Python dependencies
pip install -r requirements.txt

# Install Playwright browser
playwright install chromium

# Install frontend dependencies
cd frontend && npm install && cd ..
```

### Configuration

Edit the `.env` file in the root directory:

```env
# Required: Get your free API key from https://console.groq.com
GROQ_API_KEY=your_groq_api_key_here

# Vector Store (default: local ChromaDB)
VECTOR_STORE_TYPE=chroma
CHROMA_PERSIST_DIRECTORY=./data/chroma_db

# Database (optional - uses in-memory by default)
USE_FAKE_DB=1
```

### Run the Application

**Terminal 1 - Backend:**
```powershell
# Windows
.\.venv\Scripts\Activate.ps1
python run_uvicorn.py

# Linux/Mac
source .venv/bin/activate
python run_uvicorn.py
```

**Terminal 2 - Frontend:**
```powershell
cd frontend
npm run dev
```

### Access

| Service | URL | Description |
|---------|-----|-------------|
| ğŸ’¬ **Chat UI** | http://localhost:5173 | Main chat interface |
| âš™ï¸ **Admin Panel** | Click Admin button in UI | Crawl websites and ingest content |
| ğŸ“š **API Docs** | http://localhost:8000/docs | Swagger documentation |

---

## ğŸ“ Project Structure

```
Web-Crawling/
â”œâ”€â”€ .env                      # API keys & configuration
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ run_uvicorn.py            # Windows-compatible server runner
â”‚
â”œâ”€â”€ backend/                  # ğŸ”™ PYTHON BACKEND
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI app entry point
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ chat.py       # POST /api/chat
â”‚   â”‚       â”œâ”€â”€ health.py     # GET /api/health
â”‚   â”‚       â””â”€â”€ ingestion.py  # POST /api/ingest
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ chatbot_orchestrator.py  # RAG pipeline
â”‚   â”‚   â”œâ”€â”€ llm_service.py           # Groq/OpenAI integration
â”‚   â”‚   â”œâ”€â”€ embeddings.py            # Local embeddings
â”‚   â”‚   â””â”€â”€ vector_store.py          # ChromaDB/Pinecone
â”‚   â””â”€â”€ data_ingestion/
â”‚       â”œâ”€â”€ scraper.py        # Playwright web crawler
â”‚       â”œâ”€â”€ chunker.py        # Text splitter
â”‚       â””â”€â”€ pipeline.py       # Ingestion orchestrator
â”‚
â”œâ”€â”€ frontend/                 # ğŸ¨ REACT FRONTEND
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.tsx           # Main app
â”‚       â””â”€â”€ components/
â”‚           â”œâ”€â”€ ClientChat.tsx    # Chat interface
â”‚           â””â”€â”€ AdminPanel.tsx    # Ingestion UI
â”‚
â””â”€â”€ data/
    â””â”€â”€ chroma_db/            # ğŸ’¾ Local vector storage
```

---

## ğŸ”„ How It Works

### 1. Ingestion Phase (Crawl a website)
```
Website URL
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SCRAPER      â”‚  Playwright visits pages, extracts HTML
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    CHUNKER      â”‚  Splits text into ~1000 char chunks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EMBEDDINGS    â”‚  Converts each chunk to 384-dim vector
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VECTOR STORE   â”‚  Stores vectors in ChromaDB
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Query Phase (Ask a question)
```
User Question: "What is your pricing?"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EMBEDDINGS    â”‚  Convert question to vector
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VECTOR SEARCH  â”‚  Find top 5 most similar chunks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LLM        â”‚  Generate response using context + question
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
"Based on the website, pricing starts at $9/month..."
```

---

## ğŸ“¡ API Reference

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/chat` | Send message & get AI response |
| `GET` | `/api/chat/stats` | Get system statistics |
| `POST` | `/api/ingest` | Start website crawl |
| `GET` | `/api/ingest/{job_id}` | Check crawl status |
| `GET` | `/api/health` | Health check |

### Example: Start Crawling

```bash
curl -X POST http://localhost:8000/api/ingest \
  -H "Content-Type: application/json" \
  -d '{"url": "https://docs.python.org", "max_pages": 10, "max_depth": 2}'
```

### Example: Chat

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is this website about?"}'
```

---

## ğŸ› ï¸ Tech Stack

| Category | Technology |
|----------|------------|
| **Frontend** | React 18 + TypeScript + Vite |
| **Backend** | FastAPI + Python 3.10 |
| **LLM** | Groq (Llama 3.3 70B) - Free tier |
| **Embeddings** | Sentence Transformers (local) |
| **Vector DB** | ChromaDB (local) |
| **Web Scraping** | Playwright + BeautifulSoup |

---

## ğŸ’° Cost

| Component | Cost |
|-----------|------|
| LLM (Groq) | **$0** (free tier) |
| Embeddings | **$0** (local) |
| Vector DB | **$0** (local) |
| **Total** | **$0/month** |

---

## ğŸ”§ Configuration Options

| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_PROVIDER` | `groq`, `openai`, or `google` | `groq` |
| `GROQ_API_KEY` | Your Groq API key | Required |
| `VECTOR_STORE_TYPE` | `chroma` or `pinecone` | `chroma` |
| `USE_FAKE_DB` | Use in-memory DB (no MongoDB) | `1` |
| `TOP_K` | Chunks to retrieve | `5` |
| `CHUNK_SIZE` | Characters per chunk | `1000` |

---

## ğŸ§ª CLI Usage

You can also run the crawler from command line:

```bash
# Crawl a website
python -m backend.data_ingestion.pipeline --url https://example.com --max-pages 20

# With reset (clear existing data)
python -m backend.data_ingestion.pipeline --url https://example.com --reset
```

---

## ğŸ“ License

MIT License - Free to use and modify.

---

## ğŸ‘¤ Author

**Udai Senevirathne**  
GitHub: [@Udai-Senevirathne](https://github.com/Udai-Senevirathne)
